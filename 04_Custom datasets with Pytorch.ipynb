{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d6ea5a9",
   "metadata": {},
   "source": [
    "Notes: 这一章 与学习目的关系不大，将采用阅读的方式完成."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc64617d",
   "metadata": {},
   "source": [
    "https://www.learnpytorch.io/04_pytorch_custom_datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb86141",
   "metadata": {},
   "source": [
    "“What is a custom dataset?\" \n",
    "\n",
    "*PyTorch includes many existing functions to load in various custom datasets in the [`TorchVision`](https://pytorch.org/vision/stable/index.html), [`TorchText`](https://pytorch.org/text/stable/index.html), [`TorchAudio`](https://pytorch.org/audio/stable/index.html) and [`TorchRec`](https://pytorch.org/torchrec/) domain libraries.*\n",
    "\n",
    "But sometimes these existing functions may not be enough.\n",
    "\n",
    "In that case, we can always subclass `torch.utils.data.Dataset` and customize it to our liking.  A custom dataset is specific dataset of own our own problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442297e1",
   "metadata": {},
   "source": [
    "## What we're going to cover\n",
    "\n",
    "We're going to be applying the PyTorch Workflow we covered in [notebook 01](https://www.learnpytorch.io/01_pytorch_workflow/) and [notebook 02](https://www.learnpytorch.io/02_pytorch_classification/) to a computer vision problem.\n",
    "\n",
    "But instead of using an in-built PyTorch dataset, we're going to be using our own dataset of pizza, steak and sushi images.\n",
    "\n",
    "The goal will be to load these images and then build a model to train and predict on them.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pytorch-food-vision-layout.png\" alt=\"building a pipeline to load in food images and then building a pytorch model to classify those food images\" width=800 />\n",
    "\n",
    "*What we're going to build. We'll use `torchvision.datasets` as well as our own custom `Dataset` class to load in images of food and then we'll build a PyTorch computer vision model to hopefully be able to classify them.*\n",
    "\n",
    "Specifically, we're going to cover:\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **0. Importing PyTorch and setting up device-agnostic code** | Let's get PyTorch loaded and then follow best practice to setup our code to be device-agnostic.  |\n",
    "| **1. Get data** | We're going to be using our own **custom dataset** of pizza, steak and sushi images. |\n",
    "| **2. Become one with the data (data preparation)** | At the beginning of any new machine learning problem, it's paramount to understand the data you're working with. Here we'll take some steps to figure out what data we have. |\n",
    "| **3. Transforming data** |Often, the data you get won't be 100% ready to use with a machine learning model, here we'll look at some steps we can take to *transform* our images so they're ready to be used with a model. | \n",
    "| **4. Loading data with `ImageFolder` (option 1)** | PyTorch has many in-built data loading functions for common types of data. `ImageFolder` is helpful if our images are in standard image classification format. |\n",
    "| **5. Loading image data with a custom `Dataset`** | What if PyTorch didn't have an in-built function to load data with? This is where we can build our own custom subclass of `torch.utils.data.Dataset`. |\n",
    "| **6. Other forms of transforms (data augmentation)** | Data augmentation is a common technique for expanding the diversity of your training data. Here we'll explore some of `torchvision`'s in-built data augmentation functions. |\n",
    "| **7. Model 0: TinyVGG without data augmentation** | By this stage, we'll have our data ready, let's build a model capable of fitting it. We'll also create some training and testing functions for training and evaluating our model. |\n",
    "| **8. Exploring loss curves** | Loss curves are a great way to see how your model is training/improving over time. They're also a good way to see if your model is **underfitting** or **overfitting**. |\n",
    "| **9. Model 1: TinyVGG with data augmentation** | By now, we've tried a model *without*, how about we try one *with* data augmentation? |\n",
    "| **10. Compare model results** | Let's compare our different models' loss curves and see which performed better and discuss some options for improving performance. |\n",
    "| **11. Making a prediction on a custom image** | Our model is trained to on a dataset of pizza, steak and sushi images. In this section we'll cover how to use our trained model to predict on an image *outside* of our existing dataset. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb5cd1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
