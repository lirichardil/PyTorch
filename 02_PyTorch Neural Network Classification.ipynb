{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7e0b422",
   "metadata": {},
   "source": [
    "## What is a Classification Problem? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89294d84",
   "metadata": {},
   "source": [
    "Neural Network Classification: Predicting things.\n",
    "\n",
    "Linear Regression: Predicting number.\n",
    "\n",
    "https://www.learnpytorch.io/02_pytorch_classification/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bffdb4",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "A classification problem involves predicting whether something is one thing or another.\n",
    "\n",
    "For example, you might want to:\n",
    "\n",
    "For example, you might want to:\n",
    "\n",
    "| Problem type | What is it? | Example |\n",
    "| ----- | ----- | ----- |\n",
    "| **Binary classification** | Target can be one of two options, e.g. yes or no | Predict whether or not someone has heart disease based on their health parameters. |\n",
    "| **Multi-class classification** | Target can be one of more than two options | Decide whether a photo is of food, a person or a dog. |\n",
    "| **Multi-label classification** | Target can be assigned more than one option | Predict what categories should be assigned to a Wikipedia article (e.g. mathematics, science & philosophy). |\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/02-different-classification-problems.png\" alt=\"various different classification in machine learning such as binary classification, multiclass classification and multilabel classification\" width=900/>\n",
    "</div>\n",
    "    \n",
    "Classification, along with regression (predicting a number, covered in [notebook 01](https://www.learnpytorch.io/01_pytorch_workflow/)) is one of the most common types of machine learning problems.\n",
    "\n",
    "In this notebook, we're going to work through a couple of different classification problems with PyTorch. \n",
    "\n",
    "In other words, taking a set of inputs and predicting what class those set of inputs belong to.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6e2dca",
   "metadata": {},
   "source": [
    "### The inputs and outputs of Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acf75da",
   "metadata": {},
   "source": [
    "Inputs:  \n",
    "\n",
    "- Pictures (pixel values: Width x Height x Color Chanel[R,G,B])\n",
    "- Shapes: [Batch Size(usually 32), Color Channels, Width, Height] \n",
    "\n",
    "Outputs:  \n",
    "- Predicted value of each categories (proberbilities)\n",
    "- [Shapes = Option of Classes we have]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6e7ce4",
   "metadata": {},
   "source": [
    "## What we're going to cover"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a1b2c0",
   "metadata": {},
   "source": [
    "Specifically, we're going to cover:\n",
    "\n",
    "| **Topic** | **Contents** |\n",
    "| ----- | ----- |\n",
    "| **0. Architecture of a classification neural network** | Neural networks can come in almost any shape or size, but they typically follow a similar floor plan. |\n",
    "| **1. Getting binary classification data ready** | Data can be almost anything but to get started we're going to create a simple binary classification dataset. |\n",
    "| **2. Building a PyTorch classification model** | Here we'll create a model to learn patterns in the data, we'll also choose a **loss function**, **optimizer** and build a **training loop** specific to classification. | \n",
    "| **3. Fitting the model to data (training)** | We've got data and a model, now let's let the model (try to) find patterns in the (**training**) data. |\n",
    "| **4. Making predictions and evaluating a model (inference)** | Our model's found patterns in the data, let's compare its findings to the actual (**testing**) data. |\n",
    "| **5. Improving a model (from a model perspective)** | We've trained and evaluated a model but it's not working, let's try a few things to improve it. |\n",
    "| **6. Non-linearity** | So far our model has only had the ability to model straight lines, what about non-linear (non-straight) lines? |\n",
    "| **7. Replicating non-linear functions** | We used **non-linear functions** to help model non-linear data, but what do these look like? |\n",
    "| **8. Putting it all together with multi-class classification** | Let's put everything we've done so far for binary classification together with a multi-class classification problem. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784ac25",
   "metadata": {},
   "source": [
    "## 0. Architecture of a classification neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb13aa41",
   "metadata": {},
   "source": [
    "Before we get into writing code, let's look at the general architecture of a classification neural network.\n",
    "\n",
    "| **Hyperparameter** | **Binary Classification** | **Multiclass classification** |\n",
    "| --- | --- | --- |\n",
    "| **Input layer shape** (`in_features`) | Same as number of features (e.g. 5 for age, sex, height, weight, smoking status in heart disease prediction) | Same as binary classification |\n",
    "| **Hidden layer(s)** | Problem specific, minimum = 1, maximum = unlimited | Same as binary classification |\n",
    "| **Neurons per hidden layer** | Problem specific, generally 10 to 512 | Same as binary classification |\n",
    "| **Output layer shape** (`out_features`) | 1 (one class or the other) | 1 per class (e.g. 3 for food, person or dog photo) |\n",
    "| **Hidden layer activation** | Usually [ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html#torch.nn.ReLU) (rectified linear unit) but [can be many others](https://en.wikipedia.org/wiki/Activation_function#Table_of_activation_functions) | Same as binary classification |\n",
    "| **Output activation** | [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function) ([`torch.sigmoid`](https://pytorch.org/docs/stable/generated/torch.sigmoid.html) in PyTorch)| [Softmax](https://en.wikipedia.org/wiki/Softmax_function) ([`torch.softmax`](https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html) in PyTorch) |\n",
    "| **Loss function** | [Binary crossentropy](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression) ([`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) in PyTorch) | Cross entropy ([`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) in PyTorch) |\n",
    "| **Optimizer** | [SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) (stochastic gradient descent), [Adam](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) (see [`torch.optim`](https://pytorch.org/docs/stable/optim.html) for more options) | Same as binary classification |\n",
    "\n",
    "Of course, this ingredient list of classification neural network components will vary depending on the problem you're working on.\n",
    "\n",
    "But it's more than enough to get started.\n",
    "\n",
    "We're going to get hands-on with this setup throughout this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d02620e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aca00892",
   "metadata": {},
   "source": [
    "## 1. Make Classification data and get it ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c804ddaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "960e7699",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "# Make 1000 samples\n",
    "n_samples = 1000\n",
    "\n",
    "#Create circles\n",
    "X,y = make_circles(n_samples, noise=0.03, random_state=42)\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3676f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 samples of X: {X[:5]},size of X: {X.shape}\")\n",
    "print(f\"First 5 samples of y: {y[:5]},size of y: {y.shape} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543258ab",
   "metadata": {},
   "source": [
    "we can see that for X, there are two features, and Y has only 1 output(either 0 or 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d44e0403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "circles = pd.DataFrame({\"X0\":X[:,0],\"X1\":X[:,1],\"label\":y}) # input a dictionary\n",
    "\n",
    "circles.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82b6633c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with a plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(x= X[:,0],y = X[:,1],c=y,cmap=plt.cm.RdYlBu)\n",
    "plt.xlabel(\"Feature 0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6723a27",
   "metadata": {},
   "source": [
    "now we can see from the graph that in the classification problem, we are trying to classify a point based on which circles it's in. so given two number(X1,X2) -> Blue Dots or Red Dots\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8efd5a",
   "metadata": {},
   "source": [
    "### 1.1 Check input and output shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ecd9cb",
   "metadata": {},
   "source": [
    "in ML we deal with data in tensor form, and one of the most common errors is the mismatch of input/output shape. Therefore, before doing any muniplation, first check for the input and output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40f7f7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape, y.shape\n",
    "# notice that the second number of X's shape is 2, meaning there are 2 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0cc9386",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting to tensors\n",
    "X_sample = X[0]\n",
    "y_sample = y[0]\n",
    "\n",
    "print(f\"Values for one sample of X: {X_sample} and the same for y: {y_sample}\")\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f20111e",
   "metadata": {},
   "source": [
    "### 1.2 Turn data into tensors and create train and test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db5c0cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import torch \n",
    "torch.__version__ #at least 1.10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86eb70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's check what type of data structures our data are\n",
    "type(X_sample), type(y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d5aa57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#they are numpy array. we need to convert them to tensors\n",
    "if not isinstance(X, torch.Tensor):\n",
    "    X = torch.from_numpy(X).type(torch.float32)\n",
    "if not isinstance(y, torch.Tensor):\n",
    "    y = torch.from_numpy(y).type(torch.float32)\n",
    "\n",
    "\n",
    "# the default type is double-precision float32,\n",
    "# but the dafult type of numpy is int64. so we need to convert to float32 to save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac717974",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:5], y[:5] # we should now get tensor data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "563566ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a16a7c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape\n",
    "# the 1000 data sets is set to 800 training and 200 test sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b372a06",
   "metadata": {},
   "source": [
    "## 2. Building a model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231b255",
   "metadata": {},
   "source": [
    "Let's build a model to classify our blue and red dots. \n",
    "\n",
    "To do so, we want to:\n",
    "1. Setup device agonistic code\n",
    "2. Construct a model(by subclssing  `nn.Module`)\n",
    "3. Define a loss function and optimizer\n",
    "4. Create a training and test loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e4b070c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up device- agnostic code\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7fab2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class CircleModelV0(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=2, out_features=5)\n",
    "        self.layer_2 = nn.Linear(in_features=5, out_features=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layer_2(self.layer_1(x))\n",
    "\n",
    "# Create an instance of the model and send it to the target device\n",
    "model_0 = CircleModelV0().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58546c1b",
   "metadata": {},
   "source": [
    "another way to construct a model is to use pre-built class... this allows faster model building, but of course, subclassing nn.module is more flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f517e8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = nn.Sequential(\n",
    "    nn.Linear(in_features=2, out_features=5),\n",
    "    nn.Linear(in_features=5, out_features=1)\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4681c65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.state_dict()\n",
    "# Five values for bias in layer 1 (5 neurons) and 10 values for weights (2 features * 5 neurons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dde09fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    untrained_preds = model_0(X_test.to(device)) #make sure X_test is on the same device as model\n",
    "print(f\"Lengh of untrained preds: {len(untrained_preds)}, shape: {untrained_preds.shape}\")\n",
    "print(f\"Length of test samples: {len(y_test)}, shape: {y_test.shape}\")\n",
    "print(f\"\\n untrained_preds[:5] : {untrained_preds[:5]}\")\n",
    "print(f\"\\n y_test[:5] : {y_test[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82954c5a",
   "metadata": {},
   "source": [
    "### 2.1 Set up losss function and optimizer\n",
    "\n",
    "Which loss function or optimizer should you use? \n",
    "\n",
    "This is problem specific.\n",
    "\n",
    "For regression: \n",
    "loss-function： MAE or MSE (mean absolute error or mean square error)\n",
    "\n",
    "For classification you might want binary cross entropy or categorical cross entropy(or just entropy)\n",
    "\n",
    "* as a reminder, the loss function measures how wrong your models predictions are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95e05c0",
   "metadata": {},
   "source": [
    "For example, the stochastic gradient descent optimizer (SGD, `torch.optim.SGD()`) can be used for a range of problems, and the same applies to the Adam optimizer (`torch.optim.Adam()`). \n",
    "\n",
    "| Loss function/Optimizer | Problem type | PyTorch Code |\n",
    "| ----- | ----- | ----- |\n",
    "| Stochastic Gradient Descent (SGD) optimizer | Classification, regression, many others. | [`torch.optim.SGD()`](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html) |\n",
    "| Adam Optimizer | Classification, regression, many others. | [`torch.optim.Adam()`](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) |\n",
    "| Binary cross entropy loss | Binary classification | [`torch.nn.BCELossWithLogits`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) or [`torch.nn.BCELoss`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) |\n",
    "| Cross entropy loss | Multi-class classification | [`torch.nn.CrossEntropyLoss`](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) |\n",
    "| Mean absolute error (MAE) or L1 Loss | Regression | [`torch.nn.L1Loss`](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html) | \n",
    "| Mean squared error (MSE) or L2 Loss | Regression | [`torch.nn.MSELoss`](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss) |  \n",
    "\n",
    "*Table of various loss functions and optimizers, there are more but these are some common ones you'll see.*\n",
    "\n",
    "Since we're working with a binary classification problem, let's use a binary cross entropy loss function.\n",
    "\n",
    "> **Note:** Recall a **loss function** is what measures how *wrong* your model predictions are, the higher the loss, the worse your model.\n",
    ">\n",
    "> Also, PyTorch documentation often refers to loss functions as \"loss criterion\" or \"criterion\", these are all different ways of describing the same thing.\n",
    "\n",
    "PyTorch has two binary cross entropy implementations:\n",
    "1. [`torch.nn.BCELoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html) - Creates a loss function that measures the binary cross entropy between the target (label) and input (features).\n",
    "2. [`torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) - This is the same as above except it has a sigmoid layer ([`nn.Sigmoid`](https://pytorch.org/docs/stable/generated/torch.nn.Sigmoid.html)) built-in (we'll see what this means soon).\n",
    "\n",
    "Which one should you use? \n",
    "\n",
    "The [documentation for `torch.nn.BCEWithLogitsLoss()`](https://pytorch.org/docs/stable/generated/torch.nn.BCEWithLogitsLoss.html) states that it's more numerically stable than using `torch.nn.BCELoss()` after a `nn.Sigmoid` layer. \n",
    "\n",
    "So generally, implementation 2 is a better option. However for advanced usage, you may want to separate the combination of `nn.Sigmoid` and `torch.nn.BCELoss()` but that is beyond the scope of this notebook.\n",
    "\n",
    "Knowing this, let's create a loss function and an optimizer. \n",
    "\n",
    "For the optimizer we'll use `torch.optim.SGD()` to optimize the model parameters with learning rate 0.1.\n",
    "\n",
    "> **Note:** There's a [discussion on the PyTorch forums about the use of `nn.BCELoss` vs. `nn.BCEWithLogitsLoss`](https://discuss.pytorch.org/t/bceloss-vs-bcewithlogitsloss/33586/4). It can be confusing at first but as with many things, it becomes easier with practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fdb8956f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the loss function and optimizer\n",
    "from pickletools import optimize\n",
    "\n",
    "\n",
    "loss_fn = nn.BCELoss() #requires inputs to have gone through the sigmoid activation function\n",
    "loss_fn = nn.BCEWithLogitsLoss() #for binary classification = sigmoid + binary cross entropy loss\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model_0.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f13d192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets calculate the accuracy of our untrained model\n",
    "def accuracy_fn(y_true, y_pred):\n",
    "    correct = torch.eq(y_true, y_pred).sum().item()\n",
    "    acc = (correct / len(y_pred)) * 100\n",
    "    return acc  \n",
    "\n",
    "accuracy_fn(y_true=y_test.to(device), y_pred=torch.round(torch.sigmoid(untrained_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccd8bbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81376b80",
   "metadata": {},
   "source": [
    "## 3 Train the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21e2d46",
   "metadata": {},
   "source": [
    "Here are the steps for training...\n",
    "\n",
    "1. Forward Pass\n",
    "2. Calculate the loss\n",
    "3. Zero gradients\n",
    "4. Perform Backpropagation on the loss - computes the gradient of the loss with respect for every model paramterr to be updated. This is known as backpropagation\n",
    "\n",
    "5. Step the optimizer(gradient descent) - update the parameters with respect to the loss gradients in order to improve them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713e83b2",
   "metadata": {},
   "source": [
    "### 3.1 Going from raw logits -> prediction probabilities -> prediction labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe6af2",
   "metadata": {},
   "source": [
    "Our model outputs are going to be raw **logits**\n",
    "\n",
    "We can convert these **logits** into prediction probabilities by passing them to some kind of activation funct (e.g. sigmoid for binary and softmax for multiclass classification)\n",
    "\n",
    "Then we can convert our model's prediction probabiliteis to ** prediction labels** by either rounding them or taking the argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dba0e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the first 5 outputs of the forward pass on the test data \n",
    "model_0.eval() # set the model to evaluation mode\n",
    "with torch.inference_mode():\n",
    "    y_logits = model_0(X_test.to(device))[:5]\n",
    "    # without passing in any activation function, just passing through the layers...\n",
    "y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3faa3726",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[:5] #to compare with y_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "31d2c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the sigmoid activation function on our model logits into prediction probabilities\n",
    "y_pred_probs = torch.sigmoid(y_logits) \n",
    "# output how likely the model thinks the output is class 1\n",
    "y_pred_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f1dba",
   "metadata": {},
   "source": [
    "For our prediction probability values, we need to perform a range-style round on them\n",
    "* if `y_pred_probs` >= 0.5 -> y = 1\n",
    "* otherwise y = 0\n",
    "\n",
    "this rule can be adjusted to see how stick(certain) you want to be"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4190e891",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can pass these to the round function. \n",
    "y_preds = torch.round(y_pred_probs)\n",
    "\n",
    "# In full， in line  (logis-> pred probs -> pred labels)\n",
    "y_pred_labels = torch.round(torch.sigmoid(model_0(X_test.to(device))[:5]))\n",
    "\n",
    "print(torch.eq(y_preds.squeeze(), y_pred_labels.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d542398b",
   "metadata": {},
   "source": [
    "### 3.2 Building a training and testing loop for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05af422b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "# there is also a cuda seed ..\n",
    "torch.cuda.manual_seed(42) # doing operation on cuda device\n",
    "\n",
    "epochs = 100 \n",
    "\n",
    "#put data to target device\n",
    "X_train,y_train = X_train.to(device),y_train.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a70865",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c055da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
